<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Yi Ren (‰ªªÊÑè) - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Yi Ren (‰ªªÊÑè)">
<meta property="og:title" content="Yi Ren (‰ªªÊÑè)">


  <link rel="canonical" href="https://rayeren.github.io/">
  <meta property="og:url" content="https://rayeren.github.io/">



  <meta property="og:description" content="Focusing on video and speech large generation model.">







  <meta name="google-site-verification" content="b_BbOIt7d513ZXYDw-YRMicO6O5tthYiouqWDdWjGTU" />


  <meta name="msvalidate.01" content="B5AA590E7B9C03956A6F7DEC0F776211">


  <meta name="baidu-site-verification" content="code-ZxFK3f4pfb" />

<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/#about-me">About Me</a></li>
          
            <li class="masthead__menu-item"><a href="/#-news">News</a></li>
          
            <li class="masthead__menu-item"><a href="/#-publications">Publications</a></li>
          
            <li class="masthead__menu-item"><a href="/#-honors-and-awards">Honors and Awards</a></li>
          
            <li class="masthead__menu-item"><a href="/#-educations">Educations</a></li>
          
            <li class="masthead__menu-item"><a href="/#-invited-talks">Invited Talks</a></li>
          
            <li class="masthead__menu-item"><a href="/#-internships">Internships</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="images/ry_profile.jpeg" class="author__avatar" alt="Yi Ren (‰ªªÊÑè)">
  </div>

  <div class="author__content">
    <h3 class="author__name">Yi Ren (‰ªªÊÑè)</h3>
    <p class="author__bio"></p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;">Focusing on video and speech large generation model.</div></li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Singapore</li>
      
      
      
      
        <li><a href="mailto:rayeren613@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
      
      
      
        <li><a href="https://www.linkedin.com/in/rayeren"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
      
      
        <li><a href="https://dblp.org/pid/75/6568-6.html"><i class="ai ai-dblp ai-fw" aria-hidden="true"></i> DBLP</a></li>
      
      
      
      
      
      
        <li><a href="https://github.com/RayeRen"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=4FA6C0AAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
        <li><a href="https://orcid.org/0000-0002-9160-3848"><i class="ai ai-orcid-square ai-fw"></i> ORCID</a></li>
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="mailto:rayeren613@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
      
      
      
        <a href="https://www.linkedin.com/in/rayeren"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i></a>
      
      
        <a href="https://dblp.org/pid/75/6568-6.html"><i class="ai ai-dblp ai-fw" aria-hidden="true"></i></a>
      
      
      
      
      
      
        <a href="https://github.com/RayeRen"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com/citations?user=4FA6C0AAAAAJ"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
        <a href="https://orcid.org/0000-0002-9160-3848"><i class="ai ai-orcid-square ai-fw"></i></a>
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            <p><span class="anchor" id="about-me"></span>
I am now working on audio-driven video generation and text-to-speech research. If you are seeking any form of <strong>academic cooperation</strong>, please feel free to email me at <a href="mailto:rayeren613@gmail.com">rayeren613@gmail.com</a>. We are hiring interns!</p>

<p>I graduated from <a href="http://ckc.zju.edu.cn/ckcen/main.htm">Chu Kochen Honors College</a>, Zhejiang University (ÊµôÊ±üÂ§ßÂ≠¶Á´∫ÂèØÊ°¢Â≠¶Èô¢) with a bachelor‚Äôs degree and from the Department of Computer Science and Technology, Zhejiang University (ÊµôÊ±üÂ§ßÂ≠¶ËÆ°ÁÆóÊú∫ÁßëÂ≠¶‰∏éÊäÄÊúØÂ≠¶Èô¢) with a master‚Äôs degree, advised by <a href="https://person.zju.edu.cn/zhaozhou">Zhou Zhao (ËµµÊ¥≤)</a>. I also collaborate with <a href="https://www.microsoft.com/en-us/research/people/xuta/">Xu Tan (Ë∞≠Êó≠)</a>, <a href="https://www.microsoft.com/en-us/research/people/taoqin/">Tao Qin (Áß¶Ê∂õ)</a> and <a href="https://www.microsoft.com/en-us/research/people/tyliu/">Tie-yan Liu (ÂàòÈìÅÂ≤©)</a> from <a href="https://www.microsoft.com/en-us/research/group/machine-learning-research-group/">Microsoft Research Asia</a> <img src="./images/microsoft_logo.svg" style="width: 4em;" /> closely.</p>

<p>I won the <a href="https://baike.baidu.com/item/%E7%99%BE%E5%BA%A6%E5%A5%96%E5%AD%A6%E9%87%91/9929412">Baidu Scholarship</a> (10 candidates worldwide each year) and <a href="https://ur.bytedance.com/scholarship">ByteDance Scholars Program</a> (10 candidates worldwide each year) in 2020 and was selected as one of <a href="https://mp.weixin.qq.com/s?__biz=MzA4NzQ5MTA2NA==&amp;mid=2653639431&amp;idx=1&amp;sn=25b6368c1954419b9090840347d9a27d&amp;chksm=8be75b90bc90d286a5af3ef8e610e822d705dc3cf4382b45e3f14489f3e7ec4fd8c95ed0eceb&amp;mpshare=1&amp;scene=2&amp;srcid=0511LMlj9Qv9DeIZAjMjYAU9&amp;sharer_sharetime=1620731348139&amp;sharer_shareid=631c113940cb81f34895aa25ab14422a#rd">the top 100 AI Chinese new stars</a> and AI Chinese New Star Outstanding Scholar (10 candidates worldwide each year).</p>

<p>My research interest includes speech synthesis, neural machine translation and automatic music generation. I have published 50+ papers <a href="https://scholar.google.com/citations?user=4FA6C0AAAAAJ"><img src="https://img.shields.io/endpoint?logo=Google%20Scholar&amp;url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2FRayeRen%2Frayeren.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&amp;labelColor=f6f6f6&amp;color=9cf&amp;style=flat&amp;label=citations" /></a> at the top international AI conferences such as NeurIPS, ICML, ICLR, KDD.</p>

<p>To promote the communication among the Chinese ML &amp; NLP community, we (along with other 11 young scholars worldwide) founded the <a href="https://space.bilibili.com/168887299">MLNLP community</a> in 2021. I am honored to be one of the chairs of the MLNLP committee.</p>

<p>If you like the template of this homepage, welcome to star and fork my open-sourced template version <a href="https://github.com/RayeRen/acad-homepage.github.io">AcadHomepage <img src="https://img.shields.io/github/stars/RayeRen/acad-homepage.github.io?style=social" alt="" /></a>.</p>

<h1 id="-news">üî• News</h1>
<ul>
  <li><em>2024.03</em>: üéâ Two papers are accepted by ICLR 2024</li>
  <li><em>2023.05</em>: üéâ Five papers are accepted by ACL 2023</li>
  <li><em>2023.01</em>: DiffSinger was introduced in <a href="https://www.bilibili.com/video/BV1uM411t7ZJ">a very popular video</a> (2000k+ views) in Bilibili!</li>
  <li><em>2023.01</em>: I join TikTok <img src="./images/tiktok.png" style="width: 6em;" /> as a speech research scientist in Singapore!</li>
  <li><em>2022.02</em>: I release a modern and responsive academic personal <a href="https://github.com/RayeRen/acad-homepage.github.io">homepage template</a>. Welcome to STAR and FORK!</li>
</ul>

<h1 id="-publications">üìù Publications</h1>
<h2 id="-speech-synthesis">üéô Speech Synthesis</h2>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">NeurIPS 2019</div><img src="images/fs.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://papers.nips.cc/paper/8580-fastspeech-fast-robust-and-controllable-text-to-speech.pdf">FastSpeech: Fast, Robust and Controllable Text to Speech</a> <br />
<strong>Yi Ren</strong>, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu</p>

    <p><a href="https://speechresearch.github.io/fastspeech/"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="4FA6C0AAAAAJ:qjMakFHDy7sC"></span></strong></p>

    <ul>
      <li>FastSpeech is the first fully parallel end-to-end speech synthesis model.</li>
      <li><strong>Academic Impact</strong>: This work is included by many famous speech synthesis open-source projects, such as <a href="https://github.com/espnet/espnet">ESPNet <img src="https://img.shields.io/github/stars/espnet/espnet?style=social" alt="" /></a>. Our work are promoted by more than 20 media and forums, such as <a href="https://mp.weixin.qq.com/s/UkFadiUBy-Ymn-zhJ95JcQ">Êú∫Âô®‰πãÂøÉ</a>„ÄÅ<a href="https://www.infoq.cn/article/tvy7hnin8bjvlm6g0myu">InfoQ</a>.</li>
      <li><strong>Industry Impact</strong>: FastSpeech has been deployed in <a href="https://techcommunity.microsoft.com/t5/azure-ai/neural-text-to-speech-extends-support-to-15-more-languages-with/ba-p/1505911">Microsoft Azure TTS service</a> and supports 49 more languages with state-of-the-art AI quality. It was also shown as a text-to-speech system acceleration example in <a href="https://resources.nvidia.com/events/GTC2020s21420">NVIDIA GTC2020</a>.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ICLR 2021</div><img src="images/fs2.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2006.04558">FastSpeech 2: Fast and High-Quality End-to-End Text to Speech</a> <br />
<strong>Yi Ren</strong>, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu</p>

    <p><a href="https://speechresearch.github.io/fastspeech2/"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="4FA6C0AAAAAJ:LkGwnXOMwfcC"></span></strong></p>
    <ul>
      <li>This work is included by many famous speech synthesis open-source projects, such as <a href="https://github.com/PaddlePaddle/PaddleSpeech">PaddlePaddle/Parakeet <img src="https://img.shields.io/github/stars/PaddlePaddle/PaddleSpeech?style=social" alt="" /></a>, <a href="https://github.com/espnet/espnet">ESPNet <img src="https://img.shields.io/github/stars/espnet/espnet?style=social" alt="" /></a> and <a href="https://github.com/pytorch/fairseq">fairseq <img src="https://img.shields.io/github/stars/pytorch/fairseq?style=social" alt="" /></a>.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ICLR 2024</div><img src="images/mega.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://openreview.net/forum?id=mvMI3N4AvD">Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis</a> \ 
Ziyue Jiang, Jinglin Liu, <strong>Yi Ren</strong>, et al.</p>

    <p><a href="https://boostprompt.github.io/boostprompt/"><strong>Project</strong></a></p>
    <ul>
      <li>This work has been deployed on many TikTok products.</li>
      <li>Advandced zero-shot voice cloning model.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">AAAI 2022</div><img src="images/diffsinger.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2105.02446">DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism</a> <br />
Jinglin Liu, Chengxi Li, <strong>Yi Ren</strong>, Feiyang Chen, Zhou Zhao</p>

    <ul>
      <li>Many <a href="https://www.bilibili.com/video/BV1be411N7JA">video demos</a> created by the <a href="https://github.com/openvpi">DiffSinger community</a> are released.</li>
      <li>
        <p>DiffSinger was introduced in <a href="https://www.bilibili.com/video/BV1uM411t7ZJ">a very popular video</a> (1600k+ views) on Bilibili!</p>
      </li>
      <li><a href="https://diffsinger.github.io/"><strong>Project</strong></a> | <a href="https://github.com/NATSpeech/NATSpeech"><img src="https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&amp;label=DiffSpeech Stars" alt="" /></a> | <a href="https://github.com/MoonInTheRiver/DiffSinger"><img src="https://img.shields.io/github/stars/MoonInTheRiver/DiffSinger?style=social&amp;label=DiffSinger Stars" alt="" /></a> | <a href="https://huggingface.co/spaces/NATSpeech/DiffSpeech"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo" alt="Hugging Face" /></a></li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">NeurIPS 2021</div><img src="images/portaspeech.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2109.15166">PortaSpeech: Portable and High-Quality Generative Text-to-Speech</a> <br />
<strong>Yi Ren</strong>, Jinglin Liu, Zhou Zhao</p>

    <p><a href="https://portaspeech.github.io/"><strong>Project</strong></a> | <a href="https://github.com/NATSpeech/NATSpeech"><img src="https://img.shields.io/github/stars/NATSpeech/NATSpeech?style=social&amp;label=Code+Stars" alt="" /></a> | <a href="https://huggingface.co/spaces/NATSpeech/PortaSpeech"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-blue?label=Demo" alt="Hugging Face" /></a></p>
  </div>
</div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2024</code> <a href="https://arxiv.org/abs/2312.11947">Emotion Rendering for Conversational Speech Synthesis with Heterogeneous Graph-Based Context Modeling</a>, Rui Liu, Yifan Hu, <strong>Yi Ren</strong>, et al. <a href="https://github.com/walker-hyf/ECSS"><img src="https://img.shields.io/github/stars/walker-hyf/ECSS?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">ICML 2023</code> <a href="https://text-to-audio.github.io/paper.pdf">Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</a>, Rongjie Huang, Jiawei Huang, Dongchao Yang, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="">CLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-Training</a>, Zhenhui Ye, Rongjie Huang, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="">FluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models</a>, Ziyue Jiang, Qian Yang, Jialong Zuo, Zhenhui Ye, Rongjie Huang, <strong>Yi Ren</strong> and Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="">Revisiting and Incorporating GAN and Diffusion Models in High-Fidelity Speech Synthesis</a>, Rongjie Huang, <strong>Yi Ren</strong>, Ziyue Jiang, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="">Improving Prosody with Masked Autoencoder and Conditional Diffusion Model For Expressive Text-to-Speech</a>, Rongjie Huang, Chunlei Zhang, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2023</code> <a href="https://openreview.net/forum?id=SbR9mpTuBn">Bag of Tricks for Unsupervised Text-to-Speech</a>, <strong>Yi Ren</strong>, Chen Zhang, Shuicheng Yan</li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2023</code> <a href="https://arxiv.org/abs/2305.17732">StyleS2ST: zero-shot style transfer for direct speech-to-speech translation</a>, Kun Song, <strong>Yi Ren</strong>, Yi Lei, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2023</code> <a href="https://arxiv.org/abs/2306.15304">GenerTTS: Pronunciation Disentanglement for Timbre and Style Generalization in Cross-Lingual Text-to-Speech</a>, Yahuan Cong, Haoyu Zhang, Haopeng Lin, Shichao Liu, Chunfeng Wang, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2022</code> <a href="">Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech</a>, Ziyue Jiang, Zhe Su, Zhou Zhao, Qian Yang, <strong>Yi Ren</strong>, et al. <a href="https://github.com/Zain-Jiang/Dict-TTS"><img src="https://img.shields.io/github/stars/Zain-Jiang/Dict-TTS?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2022</code> <a href="">GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech</a>, Rongjie Huang, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2022</code> <a href="">M4Singer: a Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus</a>, Lichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng, Jinglin Liu, <strong>Yi Ren</strong>, et al. <em>(Datasets and Benchmarks Track)</em> <a href="https://github.com/M4Singer/M4Singer"><img src="https://img.shields.io/github/stars/M4Singer/M4Singer?style=social&amp;label=Dataset+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2022</code> <a href="">ProDiff: Progressive Fast Diffusion Model for High-Quality Text-to-Speech</a>, Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, <strong>Yi Ren</strong>, <a href="https://github.com/Rongjiehuang/ProDiff"><img src="https://img.shields.io/github/stars/Rongjiehuang/ProDiff?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2022</code> <a href="https://arxiv.org/abs/2110.07468">SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation</a>, Rongjie Huang, Chenye Cui, Chen Feiayng, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2022</code> <a href="">SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech</a>, Zhenhui Ye, Zhou Zhao, <strong>Yi Ren</strong>, et al. <a href="https://github.com/yerfor/SyntaSpeech"><img src="https://img.shields.io/github/stars/yerfor/SyntaSpeech?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2022</code> <span style="color:red">(Oral)</span> <a href="">EditSinger: Zero-Shot Text-Based Singing Voice Editing System with Diverse Prosody Modeling</a>, Lichao Zhang, Zhou Zhao, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2022</code> <a href="">FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis</a>, Rongjie Huang, Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu, <strong>Yi Ren</strong>, Zhou Zhao,  <span style="color:red">(Oral)</span>, <a href="https://github.com/Rongjiehuang/FastDiff"><img src="https://img.shields.io/github/stars/Rongjiehuang/FastDiff?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">NAACL 2022</code> <a href="">A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation</a>, Kexun Zhang, Rui Wang, Xu Tan, Junliang Guo, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2022</code> <a href="https://arxiv.org/abs/2202.13066">Revisiting Over-Smoothness in Text to Speech</a>, <strong>Yi Ren</strong>, Xu Tan, Tao Qin, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2022</code> <a href="https://arxiv.org/abs/2202.13277">Learning the Beauty in Songs: Neural Singing Voice Beautifier</a>, Jinglin Liu, Chengxi Li, <strong>Yi Ren</strong>, et al. | <a href="https://github.com/MoonInTheRiver/NeuralSVB"><img src="https://img.shields.io/github/stars/MoonInTheRiver/NeuralSVB?style=social&amp;label=Code+Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">ICASSP 2022</code> <a href="https://prosospeech.github.io/">ProsoSpeech: Enhancing Prosody With Quantized Vector Pre-training in Text-to-Speech</a>, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2021</code> <a href="https://arxiv.org/abs/2106.09317">EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model</a>, Chenye Cui, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2021</code> <span style="color:red">(best student paper award candidate)</span> <a href="https://arxiv.org/abs/2106.08507">WSRGlow: A Glow-based Waveform Generative Model for Audio Super-Resolution</a>, Kexun Zhang, <strong>Yi Ren</strong>, Changliang Xu and Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">ICASSP 2021</code> <a href="https://arxiv.org/abs/2012.09547">Denoising Text to Speech with Frame-Level Noise Modeling</a>, Chen Zhang, <strong>Yi Ren</strong>, Xu Tan, et al. | <a href="https://speechresearch.github.io/denoispeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2021</code> <a href="https://arxiv.org/pdf/2112.10358">Multi-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale Corpus</a>, Rongjie Huang, Feiyang Chen, <strong>Yi Ren</strong>, et al. <span style="color:red">(Oral)</span></li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2021</code> <a href="https://www.ijcai.org/proceedings/2021/527">FedSpeech: Federated Text-to-Speech with Continual Learning</a>, Ziyue Jiang, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">KDD 2020</code> <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403249">DeepSinger: Singing Voice Synthesis with Data Mined From the Web</a>, <strong>Yi Ren</strong>, Xu Tan, Tao Qin, et al. | <a href="https://speechresearch.github.io/deepsinger/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">KDD 2020</code> <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403331">LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition</a>, Jin Xu, Xu Tan, <strong>Yi Ren</strong>, et al. | <a href="https://speechresearch.github.io/lrspeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">INTERSPEECH 2020</code> <a href="https://www.isca-speech.org/archive/Interspeech_2020/pdfs/3139.pdf">MultiSpeech: Multi-Speaker Text to Speech with Transformer</a>, Mingjian Chen, Xu Tan, <strong>Yi Ren</strong>, et al. | <a href="https://speechresearch.github.io/multispeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">ICML 2019</code> <span style="color:red">(Oral)</span> <a href="https://pdfs.semanticscholar.org/9075/a3e6271e5ef4953491488d1776527e632408.pdf">Almost Unsupervised Text to Speech and Automatic Speech Recognition</a>, <strong>Yi Ren</strong>, Xu Tan, Tao Qin, et al.  | <a href="https://speechresearch.github.io/unsuper/"><strong>Project</strong></a></li>
</ul>

<h2 id="-talkingface--avatar">üëÑ TalkingFace &amp; Avatar</h2>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ICLR 2024</div><img src="images/real3d.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://openreview.net/forum?id=7ERQPyR2eb">Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis</a>, Zhenhui Ye, Tianyun Zhong, Yi Ren, et al. <span style="color:red">(Spotlight)</span> <a href="https://real3dportrait.github.io/"><strong>Project</strong></a> | <a href="https://github.com/yerfor/Real3DPortrait"><strong>Code</strong></a></p>
  </div>
</div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2023</code> <a href="https://openreview.net/forum?id=YfwMIDhPccD">GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis</a>, Zhenhui Ye, Ziyue Jiang, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2024</code> <a href="https://arxiv.org/abs/2305.09381">AMD: Autoregressive Motion Diffusion</a>, Bo Han, Hao Peng, Minjing Dong, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2022</code> <a href="https://arxiv.org/abs/2107.06831">Parallel and High-Fidelity Text-to-Lip Generation</a>, Jinglin Liu, Zhiying Zhu, <strong>Yi Ren</strong>, et al. | <a href="https://github.com/Dianezzy/ParaLip"><img src="https://img.shields.io/github/stars/Dianezzy/ParaLip?style=social&amp;label=ParaLip Stars" alt="" /></a></li>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2022</code> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/19966">Flow-based Unconstrained Lip to Speech Generation</a>, Jinzheng He, Zhou Zhao, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2020</code> <a href="https://dl.acm.org/doi/10.1145/3394171.3413740">FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire</a>, Jinglin Liu, <strong>Yi Ren</strong>, et al.</li>
</ul>

<h2 id="-machine-translation">üìö Machine Translation</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ACL 2023</code> <a href="">AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation</a>, Rongjie Huang, Huadai Liu, Xize Cheng, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2023</code> <a href="https://openreview.net/forum?id=UVAmFAtC5ye">TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation</a>, Rongjie Huang, Jinglin Liu, Huadai Liu, <strong>Yi Ren</strong>, Lichao Zhang, Jinzheng He, Zhou Zhao</li>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2021</code> <a href="https://arxiv.org/abs/2006.07926">UWSpeech: Speech to Speech Translation for Unwritten Languages</a>, Chen Zhang, Xu Tan, <strong>Yi Ren</strong>, et al. | <a href="https://speechresearch.github.io/uwspeech/"><strong>Project</strong></a></li>
  <li><code class="language-plaintext highlighter-rouge">IJCAI 2020</code> <a href="https://www.ijcai.org/Proceedings/2020/0534.pdf">Task-Level Curriculum Learning for Non-Autoregressive Neural Machine Translation</a>, Jinglin Liu, <strong>Yi Ren</strong>, Xu Tan, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2020</code> <a href="https://www.aclweb.org/anthology/2020.acl-main.350">SimulSpeech: End-to-End Simultaneous Speech to Text Translation</a>, <strong>Yi Ren</strong>, Jinglin Liu, Xu Tan, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACL 2020</code> <a href="https://arxiv.org/abs/2004.10454">A Study of Non-autoregressive Model for Sequence Generation</a>, <strong>Yi Ren</strong>, Jinglin Liu, Xu Tan, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2019</code> <a href="https://openreview.net/forum?id=S1gUsoR9YX">Multilingual Neural Machine Translation with Knowledge Distillation</a>, Xu Tan, <strong>Yi Ren</strong>, Di He, et al.</li>
</ul>

<h2 id="-music--dance-generation">üéº Music &amp; Dance Generation</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">IEEE TMM</code> <a href="https://ieeexplore.ieee.org/document/10149095">SDMuse: Stochastic Differential Music Editing and Generation via Hybrid Representation</a>, Chen Zhang, Yi Ren, Kejun Zhang, Shuicheng Yan.</li>
  <li><code class="language-plaintext highlighter-rouge">AAAI 2021</code> <a href="https://arxiv.org/abs/2012.05168">SongMASS: Automatic Song Writing with Pre-training and Alignment Constraint</a>, Zhonghao Sheng, Kaitao Song, Xu Tan, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2020</code> <span style="color:red">(Oral)</span> <a href="https://dl.acm.org/doi/10.1145/3394171.3413721">PopMAG: Pop Music Accompaniment Generation</a>, <strong>Yi Ren</strong>, Jinzheng He, Xu Tan, et al. | <a href="https://speechresearch.github.io/popmag/"><strong>Project</strong></a></li>
</ul>

<h2 id="-generative-model">üßë‚Äçüé® Generative Model</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ICLR 2022</code> <a href="https://openreview.net/forum?id=PlKWVd2yBkY">Pseudo Numerical Methods for Diffusion Models on Manifolds</a>, Luping Liu, <strong>Yi Ren</strong>, Zhijie Lin, Zhou Zhao | <a href="https://github.com/luping-liu/PNDM"><img src="https://img.shields.io/github/stars/luping-liu/PNDM?style=social&amp;label=Code+Stars" alt="" /></a> | <a href="https://paperswithcode.com/sota/image-generation-on-celeba-64x64?p=pseudo-numerical-methods-for-diffusion-models-1"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/pseudo-numerical-methods-for-diffusion-models-1/image-generation-on-celeba-64x64" alt="PWC" /></a></li>
</ul>

<h2 id="others">Others</h2>
<ul>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2023</code> <a href="https://openreview.net/forum?id=Rp4PA0ez0m">Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective</a>, Pengfei Wei, Lingdong Kong, Xinghua Qu, <strong>Yi Ren</strong>, et al.</li>
  <li><code class="language-plaintext highlighter-rouge">ACM-MM 2022</code> <a href="">Video-Guided Curriculum Learning for Spoken Video Grounding</a>, Yan Xia, Zhou Zhao, Shangwei Ye, Yang Zhao, Haoyuan Li, <strong>Yi Ren</strong></li>
</ul>

<h1 id="-honors-and-awards">üéñ Honors and Awards</h1>
<ul>
  <li><em>2021.10</em> Tencent Scholarship (Top 1%)</li>
  <li><em>2021.10</em> National Scholarship (Top 1%)</li>
  <li><em>2020.12</em> <a href="https://baike.baidu.com/item/%E7%99%BE%E5%BA%A6%E5%A5%96%E5%AD%A6%E9%87%91/9929412">Baidu Scholarship</a> (10 students in the world each year)</li>
  <li><em>2020.12</em> <a href="https://mp.weixin.qq.com/s?__biz=MzA4NzQ5MTA2NA==&amp;mid=2653639431&amp;idx=1&amp;sn=25b6368c1954419b9090840347d9a27d&amp;chksm=8be75b90bc90d286a5af3ef8e610e822d705dc3cf4382b45e3f14489f3e7ec4fd8c95ed0eceb&amp;mpshare=1&amp;scene=2&amp;srcid=0511LMlj9Qv9DeIZAjMjYAU9&amp;sharer_sharetime=1620731348139&amp;sharer_shareid=631c113940cb81f34895aa25ab14422a#rd">AI Chinese new stars</a> (100 worldwide each year)</li>
  <li><em>2020.12</em> <a href="https://mp.weixin.qq.com/s?__biz=MzA4NzQ5MTA2NA==&amp;mid=2653639431&amp;idx=1&amp;sn=25b6368c1954419b9090840347d9a27d&amp;chksm=8be75b90bc90d286a5af3ef8e610e822d705dc3cf4382b45e3f14489f3e7ec4fd8c95ed0eceb&amp;mpshare=1&amp;scene=2&amp;srcid=0511LMlj9Qv9DeIZAjMjYAU9&amp;sharer_sharetime=1620731348139&amp;sharer_shareid=631c113940cb81f34895aa25ab14422a#rd">AI Chinese New Star Outstanding Scholar</a> (10 candidates worldwide each year)</li>
  <li><em>2020.12</em> <a href="https://ur.bytedance.com/scholarship">ByteDance Scholars Program</a> (10 students in China each year)</li>
  <li><em>2020.10</em> Tianzhou Chen Scholarship (Top 1%)</li>
  <li><em>2020.10</em> National Scholarship (Top 1%)</li>
  <li><em>2015.10</em> National Scholarship (Undergraduate) (Top 1%)</li>
</ul>

<h1 id="-educations">üìñ Educations</h1>
<ul>
  <li><em>2019.06 - 2022.04</em>, Master, Zhejiang University, Hangzhou.</li>
  <li><em>2015.09 - 2019.06</em>, Undergraduate, Chu Kochen Honors College, Zhejiang Univeristy, Hangzhou.</li>
  <li><em>2012.09 - 2015.06</em>, Luqiao Middle School, Taizhou.</li>
</ul>

<h1 id="-invited-talks">üí¨ Invited Talks</h1>
<ul>
  <li><em>2022.02</em>, Hosted MLNLP seminar | <a href="https://www.bilibili.com/video/BV1wF411x7qh">[Video]</a></li>
  <li><em>2021.06</em>, Audio &amp; Speech Synthesis, Huawei internal talk</li>
  <li><em>2021.03</em>, Non-autoregressive Speech Synthesis, PaperWeekly &amp; biendata | <a href="https://www.bilibili.com/video/BV1uf4y1t7Hr/">[video]</a></li>
  <li><em>2020.12</em>, Non-autoregressive Speech Synthesis, Huawei Noah‚Äôs Ark Lab internal talk</li>
</ul>

<h1 id="-internships">üíª Internships</h1>
<ul>
  <li><em>2021.06 - 2021.09</em>, Alibaba, Hangzhou.</li>
  <li><em>2019.05 - 2020.02</em>, <a href="https://enjoymusic.ai/">EnjoyMusic</a>, Hangzhou.</li>
  <li><em>2019.02 - 2019.05</em>, <a href="https://www.yiwise.com/">YiWise</a>, Hangzhou.</li>
  <li><em>2018.08 - 2019.02</em>, <a href="https://www.microsoft.com/en-us/research/group/machine-learning-research-group/">MSRA, machine learning Group</a>, Beijing.</li>
  <li><em>2018.01 - 2018.06</em>, <a href="https://hr.163.com/zc/12-ai/index.html">NetEase, AI department</a>, Hangzhou.</li>
  <li><em>2017.08 - 2018.12</em>, DashBase (acquired by <a href="https://blogs.cisco.com/news/349511">Cisco</a>), Hangzhou.</li>
</ul>

          </section>
        </div>
      </article>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-SWFCX99KQZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "G-SWFCX99KQZ");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://cdn.jsdelivr.net/gh/rayeren/rayeren.github.io@'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            // var totalCitation = data['citedby']
            // document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>


  </body>
</html>
